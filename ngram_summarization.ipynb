{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use of ngram : It is basically used to find the most frequently occuring terms\n",
    "s = \"Natural-language processing (NLP) is an area of computer science \" \\\n",
    "    \"and artificial intelligence concerned with the interactions \" \\\n",
    "    \"between computers and human (natural) languages.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate n-grams without using packages:\n",
    "import re\n",
    "\n",
    "def generate_ngrams(s, n):\n",
    "    # Convert to lowercases\n",
    "    s = s.lower()\n",
    "    \n",
    "    # Replace all none alphanumeric characters with spaces\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    \n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    \n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_gram = generate_ngrams(s, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural language processing nlp is',\n",
       " 'language processing nlp is an',\n",
       " 'processing nlp is an area',\n",
       " 'nlp is an area of',\n",
       " 'is an area of computer',\n",
       " 'an area of computer science',\n",
       " 'area of computer science and',\n",
       " 'of computer science and artificial',\n",
       " 'computer science and artificial intelligence',\n",
       " 'science and artificial intelligence concerned',\n",
       " 'and artificial intelligence concerned with',\n",
       " 'artificial intelligence concerned with the',\n",
       " 'intelligence concerned with the interactions',\n",
       " 'concerned with the interactions between',\n",
       " 'with the interactions between computers',\n",
       " 'the interactions between computers and',\n",
       " 'interactions between computers and human',\n",
       " 'between computers and human natural',\n",
       " 'computers and human natural languages']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using nltk package\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "\n",
    "s = s.lower()\n",
    "s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "output = list(ngrams(tokens, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('natural', 'language'),\n",
       " ('language', 'processing'),\n",
       " ('processing', 'nlp'),\n",
       " ('nlp', 'is'),\n",
       " ('is', 'an'),\n",
       " ('an', 'area'),\n",
       " ('area', 'of'),\n",
       " ('of', 'computer'),\n",
       " ('computer', 'science'),\n",
       " ('science', 'and'),\n",
       " ('and', 'artificial'),\n",
       " ('artificial', 'intelligence'),\n",
       " ('intelligence', 'concerned'),\n",
       " ('concerned', 'with'),\n",
       " ('with', 'the'),\n",
       " ('the', 'interactions'),\n",
       " ('interactions', 'between'),\n",
       " ('between', 'computers'),\n",
       " ('computers', 'and'),\n",
       " ('and', 'human'),\n",
       " ('human', 'natural'),\n",
       " ('natural', 'languages')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output\n",
    "\n",
    "#The above block of code will generate the same output as the function generate_ngrams() as shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Summarization - go through  below links\n",
    "https://towardsdatascience.com/understand-text-summarization-and-create-your-own-summarizer-in-python-b26a9f09fc70\n",
    "https://towardsdatascience.com/text-summarization-in-python-76c0a41f0dc4\n",
    "https://stackabuse.com/text-summarization-with-nltk-in-python/  \n",
    "https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/\n",
    "https://rare-technologies.com/text-summarization-in-python-extractive-vs-abstractive-techniques-revisited/\n",
    "https://becominghuman.ai/text-summarization-in-5-steps-using-nltk-65b21e352b65https://towardsdatascience.com/data-scientists-guide-to-summarization-fc0db952e363 - very nice so far\n",
    "\n",
    "Text Summarization with Spicy : https://www.youtube.com/watch?v=XcZGKAF5zxg&t=90s\n",
    "https://stackabuse.com/text-summarization-with-nltk-in-python/\n",
    "https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/\n",
    "https://www.youtube.com/watch?v=8p9nHmtwk0o\n",
    "https://www.machinelearningplus.com/nlp/gensim-tutorial/\n",
    "https://towardsdatascience.com/data-scientists-guide-to-summarization-fc0db952e363 - very nice so far\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Summarisation and keyword:\n",
    "from gensim.summarization.summarizer import summarize\n",
    "from gensim.summarization import keywords\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get text from web url\n",
    "def get_only_text(url):\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    text = ' '.join(map(lambda p: p.text, soup.find_all('p')))\n",
    "    return soup.title.text, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Deep_learning\"\n",
    "text = get_only_text(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      " Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer.[39]\\n In 1995, Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm, co-developed with Peter Dayan and Hinton.[40] Many factors contribute to the slow speed, including the vanishing gradient problem analyzed in 1991 by Sepp Hochreiter.[41][42]\\n Simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s, because of artificial neural network\\'s (ANN) computational cost and a lack of understanding of how the brain wires its biological networks.\\n Both shallow and deep learning (e.g., recurrent nets) of ANNs have been explored for many years.[43][44][45] These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively.[46] Key difficulties have been analyzed, including gradient diminishing[41] and weak temporal correlation structure in neural predictive models.[47][48] Additional difficulties were the lack of training data and limited computing power.\\n Most speech recognition researchers moved away from neural nets to pursue generative modeling.\n",
      "In 2003, LSTM started to become competitive with traditional speech recognizers on certain tasks.[52] Later it was combined with connectionist temporal classification (CTC)[53] in stacks of LSTM RNNs.[54] In 2015, Google\\'s speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which they made available through Google Voice Search.[55]\\n In 2006, publications by Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh[56]\\n[57][58] showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then fine-tuning it using supervised backpropagation.[59] The papers referred to learning for deep belief nets.\\n Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR).\n",
      "Original text character length:  42363\n",
      "Summary text character length:  2356\n",
      "Compression Ratio:  94 %\n",
      "\n",
      "Keywords\n",
      ": learns\n",
      "learn\n",
      "learned\n",
      "deep learning\n",
      "layers\n",
      "layer\n",
      "layered\n",
      "image\n",
      "images\n",
      "models\n",
      "model\n",
      "modeling\n",
      "networks\n",
      "network\n",
      "recognition\n",
      "trained\n",
      "training\n",
      "train\n",
      "trains\n",
      "generative\n",
      "generally\n",
      "generalization\n",
      "general\n",
      "generating\n",
      "generate\n",
      "generalize\n"
     ]
    }
   ],
   "source": [
    "#Here by use of ratio  we can control the number of characters we want in the  summary\n",
    "#More its value more we will get the text in the summary\n",
    "\n",
    "summary=summarize(str(text), ratio=0.02)\n",
    "\n",
    "#We can use keyword on either complete text  or just summary part\n",
    "keyword=keywords(str(text), ratio=0.01)\n",
    "\n",
    "print('Summary:\\n',summary)\n",
    "print(\"Original text character length: \",len(str(text)))\n",
    "print(\"Summary text character length: \",len(summary))\n",
    "print(\"Compression Ratio: \",round(100-(100*len(summary)/len(str(text)))),\"%\")\n",
    "print('\\nKeywords\\n:',keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
